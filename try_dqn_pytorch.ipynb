{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Use device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Use device: %s\"%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--train'], dest='train', nargs=None, const=None, default=True, type=<class 'bool'>, choices=None, help=None, metavar=None)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Please tune the hyperparameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--train_ep\", default=800, type=int)\n",
    "parser.add_argument(\"--mem_capacity\", default=65000, type=int)\n",
    "parser.add_argument(\"--batch_size\", default=128, type=int)\n",
    "parser.add_argument(\"--lr\", default=0.00025, type=float)\n",
    "parser.add_argument(\"--gamma\", default=0.999, type=float)\n",
    "parser.add_argument(\"--epsilon_start\", default=1.0, type=float)\n",
    "parser.add_argument(\"--epsilon_final\", default=0.1, type=float)\n",
    "parser.add_argument(\"--epsilon_decay\", default=1000000, type=float)\n",
    "parser.add_argument(\"--target_step\", default=10000, type=int)\n",
    "parser.add_argument(\"--eval_per_ep\", default=10, type=int)\n",
    "parser.add_argument(\"--save_per_ep\", default=50, type=int)\n",
    "parser.add_argument(\"--save_dir\", default=\"./HW3/model\")\n",
    "parser.add_argument(\"--log_file\", default=\"./HW3/log.txt\")\n",
    "parser.add_argument(\"--load_model\", default=None)\n",
    "parser.add_argument(\"--train\", default=True, type=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./HW3\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size = 3, stride = 1):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w,8,4),4,2),3,1)\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h,8,4),4,2),3,1)\n",
    "        linear_input_size = convw * convh * 64\n",
    "        \n",
    "        self.fc = nn.Linear(linear_input_size, 512)\n",
    "        self.head = nn.Linear(512, outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = F.leaky_relu(self.fc(x.view(x.size(0), -1)))\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\"NOOP\", \"UP\", \"DOWN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.BATCH_SIZE = args.batch_size\n",
    "        self.GAMMA = args.gamma\n",
    "        self.EPS_START = args.epsilon_start\n",
    "        self.EPS_END = args.epsilon_final\n",
    "        self.EPS_DECAY = args.epsilon_decay\n",
    "        self.LEARN_RATE = args.lr\n",
    "        self.TARGET_UPDATE = args.target_step\n",
    "\n",
    "        self.action_dim = 3\n",
    "        self.state_dim = (84,84)\n",
    "        self.epsilon = 0.0\n",
    "        self.update_count = 0\n",
    "        \n",
    "        self.policy_net = CNN(self.state_dim[0], self.state_dim[1], self.action_dim).to(device)\n",
    "        self.target_net = CNN(self.state_dim[0], self.state_dim[1], self.action_dim).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.RMSprop(self.policy_net.parameters(), lr=self.LEARN_RATE)\n",
    "        \n",
    "        self.memory = ReplayMemory(args.mem_capacity)\n",
    "        self.interaction_steps = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        self.interaction_steps += 1\n",
    "        self.epsilon = self.EPS_END + np.maximum( (self.EPS_START-self.EPS_END) * (1 - self.interaction_steps/self.EPS_DECAY), 0)\n",
    "        if random.random() < self.epsilon:\n",
    "            return torch.tensor([[np.random.choice(np.arange(0, 3), p=[0.3, 0.6, 0.1])]], device=device, dtype=torch.long)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "    def evaluate_action(self, state, rand=0.1):\n",
    "        if random.random() < rand:\n",
    "            return torch.tensor([[np.random.choice(np.arange(0, 3), p=[0.3, 0.6, 0.1])]], device=device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            t = self.target_net(state)\n",
    "            \n",
    "            # trace and sample state\n",
    "            print_string = \"Q-value: \"+ str(t.cpu().numpy()[0]) + \\\n",
    "            \", agent's choice: \" + str(actions[t.max(1)[1].view(1, 1).cpu().numpy()[0][0]]) + \\\n",
    "            \", mean: \" + str(np.mean(t.cpu().numpy()[0]))\n",
    "            print(print_string)\n",
    "            \n",
    "            return t.max(1)[1].view(1, 1)\n",
    "\n",
    "\n",
    "    def store(self, state, action, next_state, reward, done):\n",
    "        self.memory.push(state, action, next_state, reward, done)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            print(\"[Warning] Memory data less than batch sizes!\")\n",
    "            return\n",
    "        \n",
    "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        final_mask = torch.cat(batch.done)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        next_state_values = torch.zeros(self.BATCH_SIZE,1, device=device)\n",
    "        next_state_values[final_mask.bitwise_not()] = self.target_net(non_final_next_states).max(1, True)[0].detach()\n",
    "\n",
    "        expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "    \n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_count += 1\n",
    "        if self.update_count % self.TARGET_UPDATE == 0:\n",
    "            self.update_target_net()\n",
    "\n",
    "        \n",
    "    def update_target_net(self):\n",
    "        with torch.no_grad():\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def save_model(self, path=\".\"):\n",
    "        torch.save(self.target_net.state_dict(), path+'/q_target_checkpoint_{}.pth'.format(self.interaction_steps))\n",
    "        torch.save(self.policy_net.state_dict(), path+'/q_policy_checkpoint_{}.pth'.format(self.interaction_steps))\n",
    "\n",
    "    def restore_model(self, path):\n",
    "        self.target_net.load_state_dict(torch.load(path))\n",
    "        self.policy_net.load_state_dict(torch.load(path))\n",
    "        self.target_net.eval()\n",
    "        print(\"[Info] Restore model from '%s' !\"%path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    def __init__(self):\n",
    "        self.action_dim = 3\n",
    "        self.interaction_steps = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        self.interaction_steps += 1\n",
    "        return torch.tensor( [np.random.choice(np.arange(0, 3), p=[0.3, 0.6, 0.1])], device=device, dtype=torch.long )\n",
    "\n",
    "    def evaluate_action(self, state):\n",
    "        return torch.tensor( [np.random.choice(np.arange(0, 3), p=[0.3, 0.6, 0.1])], device=device, dtype=torch.long )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_proc = T.Compose([T.ToPILImage(),\n",
    "                        T.Grayscale(), \\\n",
    "                        T.Resize((84,84), interpolation=Image.BILINEAR), \\\n",
    "                        T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atari(object):\n",
    "    def __init__(self, env_name=\"FreewayDeterministic-v4\", agent_history_length=4):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state = None\n",
    "        self.agent_history_length = agent_history_length\n",
    "\n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        frame = self.image_proc(observation).to(device)\n",
    "        self.state = frame.repeat(1,4,1,1)\n",
    "        return self.state\n",
    "\n",
    "    def image_proc(self, image):\n",
    "        return frame_proc(image)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        frame = self.image_proc(observation).to(device)\n",
    "        next_state = torch.cat( (self.state[:, 1:, :, :], frame.unsqueeze(0)), axis=1 )\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def get_render(self):\n",
    "        observation = self.env.render(mode='rgb_array')\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    num_episodes = args.train_ep\n",
    "    save_model_per_ep = args.save_per_ep\n",
    "    log_fd = open(args.log_file,'w')\n",
    "\n",
    "    ########## Training ##########\n",
    "    agent = DQN()\n",
    "    env = Atari()\n",
    "\n",
    "    if args.load_model:\n",
    "        agent.restore_model(args.load_model)\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "    \n",
    "    global_steps = 0\n",
    "    reward_box = []\n",
    "    for i_episode in range(num_episodes):\n",
    "        episode_reward = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for _ in range(10000):\n",
    "#             env.env.render()\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            \n",
    "            if done:\n",
    "                next_state = None\n",
    "\n",
    "            agent.memory.push(  state, \\\n",
    "                                action, \\\n",
    "                                next_state, \\\n",
    "                                torch.tensor([[reward]], device=device), \\\n",
    "                                torch.tensor([done], device=device, dtype=torch.bool))\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            global_steps += 1 \n",
    "\n",
    "            if global_steps > 50000:\n",
    "                agent.update()\n",
    "\n",
    "            if done:\n",
    "                print(\"Episode: %6d, interaction_steps: %6d, reward: %2d, epsilon: %f\"%(i_episode+1, agent.interaction_steps, episode_reward, agent.epsilon))\n",
    "                reward_box.append(episode_reward)\n",
    "                log_fd.write(\"Episode: %6d, interaction_steps: %6d, reward: %2d, epsilon: %f\\n\"%(i_episode+1, agent.interaction_steps, episode_reward, agent.epsilon))\n",
    "                break\n",
    "\n",
    "        if i_episode % save_model_per_ep == 0:\n",
    "            agent.save_model(args.save_dir)\n",
    "            print(\"[Info] Save model at '%s' !\"%args.save_dir)\n",
    "\n",
    "        if i_episode % args.eval_per_ep == 0:\n",
    "            test_env = Atari()\n",
    "            test_times = 5\n",
    "\n",
    "            average_reward = 0.0\n",
    "            for t_ep in range(test_times):\n",
    "                episode_reward = 0.0\n",
    "                state = test_env.reset()\n",
    "                for _ in range(10000):\n",
    "                    action = agent.evaluate_action(state)\n",
    "                    state, reward, done, _ = test_env.step(action.item())\n",
    "                    episode_reward += reward\n",
    "            average_reward += episode_reward\n",
    "            \n",
    "            print(\"Evaluation: True, episode: %6d, interaction_steps: %6d, evaluate reward: %2d\"%(i_episode+1, agent.interaction_steps, average_reward/test_times))\n",
    "            log_fd.write(\"Evaluation: True, episode: %6d, interaction_steps: %6d, evaluate reward: %2d\"%(i_episode+1, agent.interaction_steps, average_reward/test_times))\n",
    "    \n",
    "    log_fd.close()\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"DQN episode total reward\")\n",
    "    plt.plot(reward_box)\n",
    "    plt.xlabel(\"episode\")\n",
    "    plt.ylabel(\"reward\")\n",
    "    plt.savefig('HW3/training_reward.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(path):\n",
    "    agent = DQN()\n",
    "    env = Atari()\n",
    "    test_epsilon = args.epsilon_final\n",
    "    \n",
    "#     if args.load_model:\n",
    "#         agent.restore_model(args.load_model)\n",
    "#     else:\n",
    "#         test_epsilon = 1.0\n",
    "    agent.restore_model(path)\n",
    "      \n",
    "    reward_box = []\n",
    "    for i_episode in range(200):\n",
    "        episode_reward = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for _ in range(10000):\n",
    "            env.env.render() # show atari playing screen\n",
    "            action = agent.evaluate_action(state, test_epsilon)\n",
    "            \n",
    "            sys.stdin.readline() # wait until keyboard action for saving screenshots\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(\"Episode: %6d, interaction_steps: %6d, reward: %2d, epsilon: %f\"%(i_episode, agent.interaction_steps, episode_reward, test_epsilon))\n",
    "                reward_box.append(episode_reward)\n",
    "                break\n",
    "    reward_np = np.array(reward_box)\n",
    "    print(\"mean: \", np.mean(reward_np))\n",
    "    print(\"std: \", np.std(reward_np))\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"DQN episode test reward\")\n",
    "    plt.plot(reward_box)\n",
    "    plt.xlabel(\"episode\")\n",
    "    plt.ylabel(\"reward\")\n",
    "    plt.savefig('HW3/testing_reward.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train_ep TRAIN_EP]\n                             [--mem_capacity MEM_CAPACITY]\n                             [--batch_size BATCH_SIZE] [--lr LR]\n                             [--gamma GAMMA] [--epsilon_start EPSILON_START]\n                             [--epsilon_final EPSILON_FINAL]\n                             [--epsilon_decay EPSILON_DECAY]\n                             [--target_step TARGET_STEP]\n                             [--eval_per_ep EVAL_PER_EP]\n                             [--save_per_ep SAVE_PER_EP] [--save_dir SAVE_DIR]\n                             [--log_file LOG_FILE] [--load_model LOAD_MODEL]\n                             [--train TRAIN]\nipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9033 --control=9031 --hb=9030 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"7b96f557-f91b-4e0b-8c95-d410485e11b7\" --shell=9032 --transport=\"tcp\" --iopub=9034 --f=/tmp/tmp-20233X9HZ4XD3csXi.json\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "SystemExit",
     "evalue": "2",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "    # run test\n",
    "    # python DQN.py --train=False --load_model=HW3/model/q_target_checkpoint_1538048.pth\n",
    "    # run train\n",
    "    # python DQN.py\n",
    "    if args.train:\n",
    "        main()\n",
    "    else:\n",
    "        # test()\n",
    "        test('./q_target_checkpoint_1538048.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train_ep TRAIN_EP]\n                             [--mem_capacity MEM_CAPACITY]\n                             [--batch_size BATCH_SIZE] [--lr LR]\n                             [--gamma GAMMA] [--epsilon_start EPSILON_START]\n                             [--epsilon_final EPSILON_FINAL]\n                             [--epsilon_decay EPSILON_DECAY]\n                             [--target_step TARGET_STEP]\n                             [--eval_per_ep EVAL_PER_EP]\n                             [--save_per_ep SAVE_PER_EP] [--save_dir SAVE_DIR]\n                             [--log_file LOG_FILE] [--load_model LOAD_MODEL]\n                             [--train TRAIN]\nipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9033 --control=9031 --hb=9030 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"7b96f557-f91b-4e0b-8c95-d410485e11b7\" --shell=9032 --transport=\"tcp\" --iopub=9034 --f=/tmp/tmp-20233X9HZ4XD3csXi.json\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "SystemExit",
     "evalue": "2",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}